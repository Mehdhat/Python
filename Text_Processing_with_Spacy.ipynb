{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAayCTxEQlQ9"
      },
      "source": [
        "**Text Processing with Spacy**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNGjexxIQlQ_"
      },
      "source": [
        "docs = [\"Kaggle provides notebooks for python.\",\n",
        "       \"Python is an easy language.\",\n",
        "       \"Kaggle provides many datasets.\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tikN84GGQlRD"
      },
      "source": [
        "The list of ** Stop Word and Punctuations ** which will be removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWcIEh_nQlRD",
        "outputId": "eff0e390-eba4-484c-8c0f-135a3caa0ae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "# Creting List of Stop Words\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Creating list of punctuation marks\n",
        "import string\n",
        "punctuations = string.punctuation\n",
        "\n",
        "print(stop_words)\n",
        "print(\"\\n===================\\n\")\n",
        "print(punctuations)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the', 'make', 'whatever', '‘m', 'thereafter', 'very', 'whenever', 'you', 'once', 'any', 'which', 'thru', 'did', 'mostly', 'seemed', 'either', 'keep', 'about', 'beside', 'during', 'name', 'their', 'myself', 'whereafter', 'will', 'former', 'been', 'whence', 'whether', 'perhaps', \"'d\", 'before', 'yourself', 'over', 'yet', 'seeming', 'can', 'serious', 'becomes', 'from', 'using', 'really', 'much', 'ca', 'yours', 'more', 'back', '‘ll', 'then', 'whose', 'one', 'who', 'done', 'be', 'within', 'fifty', 'those', 'please', 'made', 'latterly', 'upon', 'everywhere', 'whole', 'and', 'had', 'none', 'have', 'third', 'whoever', 'meanwhile', 'except', 'is', 'together', 'same', 'though', 'such', 'mine', 'three', 'on', 'ours', 'regarding', 'all', 'somehow', 'almost', 'enough', 'to', 'were', 'her', 'after', 'whereupon', 'whom', 'often', 'six', 'give', 'top', 'into', 'namely', 'moreover', 'nevertheless', 'are', 'ever', 'should', '‘ve', 'that', 'show', 'anything', 'again', 'a', 'however', 'part', 'bottom', 'toward', 'somewhere', 'get', \"'m\", 'less', 'used', 'whereas', 'full', 'well', 'sometimes', 'me', 'do', 'or', '’d', 'besides', 'empty', 'its', 'rather', 'by', 'wherever', 'seems', 'whereby', 'side', 'due', 'above', 'per', \"'ll\", 'nine', 'since', 'each', 'hereafter', 'there', 'wherein', 'nobody', 'hence', 'anywhere', 'himself', 'thus', 'twenty', 'your', 'twelve', 'does', 'sometime', 'as', 'several', 'else', 'between', 'we', 'noone', 'became', 'but', 'am', 'might', 'nor', 'off', 'she', 'least', 'just', \"'s\", 'two', 'was', 'him', 'how', 'onto', 'along', 'beforehand', 'may', 'anyway', 'next', \"'re\", 'last', 'around', 'without', 'through', 'five', 'always', 'towards', 'here', 'hereupon', '’s', 'n’t', 'thence', 'them', 'in', 'hers', 'eight', 'of', 'an', 'thereupon', 'others', 'front', 'say', '’m', 'his', 'at', 'ten', 'against', 'among', 'go', 'everything', 'another', 'see', 'nowhere', 'never', 'even', 'than', 'why', 'something', 'these', 'call', 'when', 'although', 'this', 'our', '‘re', 'first', 'become', 'put', 're', 'every', 'both', 'with', 'until', 'some', 'take', 'seem', 'own', 'few', 'they', 'being', 'doing', 'alone', 'us', 'now', '‘s', 'he', 'while', 'ourselves', 'cannot', 'anyone', 'eleven', 'would', 'most', 'many', 'where', 'therefore', 'amount', 'fifteen', 'whither', 'beyond', 'under', 'has', 'thereby', 'further', 'could', '‘d', 'forty', 'sixty', 'quite', 'if', 'neither', 'below', '’ve', 'elsewhere', 'itself', 'n‘t', 'other', 'hundred', 'already', '’ll', 'someone', 'four', 'therein', 'throughout', 'must', 'too', 'afterwards', 'via', 'otherwise', 'still', 'not', 'up', 'my', '’re', 'out', 'move', 'nothing', 'no', 'behind', 'amongst', 'also', 'anyhow', 'unless', 'themselves', 'across', 'i', 'so', 'becoming', 'what', 'hereby', 'yourselves', 'everyone', 'herself', 'only', 'for', 'because', 'down', 'formerly', 'various', \"n't\", 'indeed', 'it', \"'ve\", 'herein', 'latter'}\n",
            "\n",
            "===================\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "en_core_web_sm is a small English model for the spaCy library. It provides basic functionality for processing English text, such as tokenization, part-of-speech tagging, and named entity recognition. This model is useful for tasks where you need a lightweight and fast solution, though it may be less accurate compared to larger models. If you're working on NLP tasks with spaCy and need something quick and efficient, en_core_web_sm is a good choice."
      ],
      "metadata": {
        "id": "pdVWvmJlW1yY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JoBvzrrQlRH"
      },
      "source": [
        "Using SpaCy library to load model of English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXwDLCxZQlRH"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PHte5bhQlRK"
      },
      "source": [
        "**SpaCy vs NLTK**\n",
        "* NLTK is built for academia, Spacy is built for industry.\n",
        "* NLTK is has many ways to do the same thing, SpaCy has only one way.\n",
        "* SpaCy is faster than NLTK.\n",
        "* More human languages are supported in NLTK than SpaCy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca9Nnhw1QlRL"
      },
      "source": [
        "Apply the NLP model of English on the given documents. The documents are tokenized and different lexical and syntactical features are assigned to the tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wldfVfeQlRL",
        "outputId": "7001949a-dee0-42ca-979d-985817ef8496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prc_docs = [nlp(doc) for doc in docs]\n",
        "print(prc_docs)\n",
        "\n",
        "#prc_docs = []\n",
        "#for doc in docs:\n",
        "  #prc_docs.append(nlp(doc))\n",
        "\n",
        "for tok in prc_docs[0]:\n",
        "  print(tok)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Kaggle provides notebooks for python., Python is an easy language., Kaggle provides many datasets.]\n",
            "Kaggle\n",
            "provides\n",
            "notebooks\n",
            "for\n",
            "python\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSwXTkFAQlRO"
      },
      "source": [
        "Transforming words into their root words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0jjT1mjQlRP",
        "outputId": "77a8a3c3-801a-4980-f125-b5ee2fd1e7db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Before: \", prc_docs)\n",
        "token_docs = [ [tok.lemma_.lower().strip() for tok in prc_doc] for prc_doc in prc_docs]\n",
        "\n",
        "#token_docs = []\n",
        "#for prc_doc in prc_docs:\n",
        "  #for tok in prc_doc:\n",
        "    #token_docs.append(tok.lemma_.lower().strip())\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nAfter: \", token_docs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:  [Kaggle provides notebooks for python., Python is an easy language., Kaggle provides many datasets.]\n",
            "\n",
            "After:  [['kaggle', 'provide', 'notebook', 'for', 'python', '.'], ['python', 'be', 'an', 'easy', 'language', '.'], ['kaggle', 'provide', 'many', 'dataset', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZk4RtsUQlRT"
      },
      "source": [
        "Removing stop words and punctuations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn3CU3VgQlRU",
        "outputId": "d81ddac2-a59e-4c1b-eac9-c555769b3f5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "token_docs = [ [tok for tok in token_doc if (tok not in stop_words and tok not in punctuations)] for token_doc in token_docs]\n",
        "print(\"Before: \", token_docs)\n",
        "print(\"\\nAfter: \",token_docs)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:  [['kaggle', 'provide', 'notebook', 'python'], ['python', 'easy', 'language'], ['kaggle', 'provide', 'dataset']]\n",
            "\n",
            "After:  [['kaggle', 'provide', 'notebook', 'python'], ['python', 'easy', 'language'], ['kaggle', 'provide', 'dataset']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnYhzD4bQlRX"
      },
      "source": [
        "Rebuilding the strings that can be passed to Vectorizer(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "wDaVFMjWQlRX",
        "outputId": "cf9c6def-f90a-44e4-f056-aaadc91a2d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "s = ''\n",
        "docs = []\n",
        "for token_doc in token_docs:\n",
        "    #for token in token_doc:\n",
        "    #    s += token + ' '\n",
        "    #docs.append(s)\n",
        "    #s = ''\n",
        "    docs.append(' '.join(token_doc))\n",
        "print(docs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kaggle provide notebook python', 'python easy language', 'kaggle provide dataset']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2gWjCzrQlRa"
      },
      "source": [
        "Transforming the list of tokens to vectors having count of words as the dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM4kLfxsQlRa",
        "outputId": "a5ee5a5a-5d70-47f3-b087-faa4ebecb6ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "print(\"Feature Labels\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(\"Sparse Matrix\")\n",
        "print(X)\n",
        "print(\"Dense Matrix\")\n",
        "print(X.todense())\n",
        "\n",
        "#vectorizer.vocabulary_\n",
        "#print(X.toarray())\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Labels\n",
            "['dataset' 'easy' 'kaggle' 'language' 'notebook' 'provide' 'python']\n",
            "Sparse Matrix\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 6)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 3)\t1\n",
            "  (2, 2)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 0)\t1\n",
            "Dense Matrix\n",
            "[[0 0 1 0 1 1 1]\n",
            " [0 1 0 1 0 0 1]\n",
            " [1 0 1 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfPpIsDeSE05",
        "outputId": "07e9759f-598b-48dc-90af-5c5d7a57014f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vectorizer2 = CountVectorizer(max_features=4, ngram_range=(1,2))\n",
        "X2 = vectorizer2.fit_transform(docs)\n",
        "print(\"Feature Labels\")\n",
        "print(vectorizer2.get_feature_names_out())\n",
        "print(\"Sparse Matrix\")\n",
        "print(X2)\n",
        "print(\"Dense Matrix\")\n",
        "print(X2.todense())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Labels\n",
            "['kaggle' 'kaggle provide' 'provide' 'python']\n",
            "Sparse Matrix\n",
            "  (0, 0)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 1)\t1\n",
            "  (1, 3)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 2)\t1\n",
            "  (2, 1)\t1\n",
            "Dense Matrix\n",
            "[[1 1 1 1]\n",
            " [0 0 0 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDO-Ad1TQlRd"
      },
      "source": [
        "It multiplies the Term Frequency (TF) by Inverse Document Frequency (IDF).\n",
        "IDF reduced the weight of those terms/words that occur in majority of the documents.\n",
        "IDFw = Count of all documents / Count of the documents in which the word w appears\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKtKXEUQlRd",
        "outputId": "b941f66e-5b09-4635-a19d-a57ae2b011db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "print(\"Feature Labels\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(\"Sparse Matrix\")\n",
        "print(X)\n",
        "print(\"Dense Matrix\")\n",
        "print(X.todense())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Labels\n",
            "['dataset' 'easy' 'kaggle' 'language' 'notebook' 'provide' 'python']\n",
            "Sparse Matrix\n",
            "  (0, 6)\t0.4598535287588349\n",
            "  (0, 4)\t0.6046521283053111\n",
            "  (0, 5)\t0.4598535287588349\n",
            "  (0, 2)\t0.4598535287588349\n",
            "  (1, 3)\t0.6227660078332259\n",
            "  (1, 1)\t0.6227660078332259\n",
            "  (1, 6)\t0.4736296010332684\n",
            "  (2, 0)\t0.680918560398684\n",
            "  (2, 5)\t0.5178561161676974\n",
            "  (2, 2)\t0.5178561161676974\n",
            "Dense Matrix\n",
            "[[0.         0.         0.45985353 0.         0.60465213 0.45985353\n",
            "  0.45985353]\n",
            " [0.         0.62276601 0.         0.62276601 0.         0.\n",
            "  0.4736296 ]\n",
            " [0.68091856 0.         0.51785612 0.         0.         0.51785612\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uF39ZfaSt01",
        "outputId": "27a17131-ff08-4036-8bb6-a2e33097da60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vectorizer2 = TfidfVectorizer(max_features=4, ngram_range=(1,2))\n",
        "X2 = vectorizer2.fit_transform(docs)\n",
        "print(\"Feature Labels\")\n",
        "print(vectorizer2.get_feature_names_out())\n",
        "print(\"Sparse Matrix\")\n",
        "print(X2)\n",
        "print(\"Dense Matrix\")\n",
        "print(X2.todense())\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Labels\n",
            "['kaggle' 'kaggle provide' 'provide' 'python']\n",
            "Sparse Matrix\n",
            "  (0, 1)\t0.5\n",
            "  (0, 3)\t0.5\n",
            "  (0, 2)\t0.5\n",
            "  (0, 0)\t0.5\n",
            "  (1, 3)\t1.0\n",
            "  (2, 1)\t0.5773502691896257\n",
            "  (2, 2)\t0.5773502691896257\n",
            "  (2, 0)\t0.5773502691896257\n",
            "Dense Matrix\n",
            "[[0.5        0.5        0.5        0.5       ]\n",
            " [0.         0.         0.         1.        ]\n",
            " [0.57735027 0.57735027 0.57735027 0.        ]]\n"
          ]
        }
      ]
    }
  ]
}