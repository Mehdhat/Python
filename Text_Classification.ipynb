{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification"
      ],
      "metadata": {
        "id": "Dmkd5B8UL-xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading and exploring Brown Corpus"
      ],
      "metadata": {
        "id": "i3QmwLCPMJoN"
      }
    },
    {
      "metadata": {
        "id": "2oM-1AhQbSJR",
        "outputId": "4ff849fb-ba57-400f-f0af-0d28287ed954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X and Y will have input and label. Xdocwords will have words/tokens present in each text document."
      ],
      "metadata": {
        "id": "3v9hQHsqMOu0"
      }
    },
    {
      "metadata": {
        "id": "KuPqvDbcdhkT"
      },
      "cell_type": "code",
      "source": [
        "Xdocwords = []\n",
        "Y = []\n",
        "\n",
        "\n",
        "for category in brown.categories():\n",
        "  for fileid in brown.fileids(category):\n",
        "    Xdocwords.append(brown.words(fileid))\n",
        "    Y.append(category)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explrong the values in Y and Sdocwords"
      ],
      "metadata": {
        "id": "vYs1Bv3fMtnN"
      }
    },
    {
      "metadata": {
        "id": "DxNdHFZnedDi",
        "outputId": "5adc1e8c-7267-4fe2-ca85-0f420007c7f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "print(Y[100])\n",
        "print(Xdocwords[100][0:20])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "belles_lettres\n",
            "['In', 'a', 'pessimistic', 'assessment', 'of', 'the', 'cold', 'war', ',', 'Eden', 'declared', ':', '``', 'There', 'must', 'be', 'much', 'closer', 'unity', 'within']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1aoRzV8l2_61",
        "outputId": "2978cd6e-1cc1-4f65-f4f8-c70b8dfd5fc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "len(Xdocwords)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating string that will be used by the vectorizer"
      ],
      "metadata": {
        "id": "S44NO95ZNCMq"
      }
    },
    {
      "metadata": {
        "id": "WLzYeirH09Ig"
      },
      "cell_type": "code",
      "source": [
        "Xdoc = []\n",
        "str = \"\"\n",
        "for doc in Xdocwords:\n",
        " # str = \"\"\n",
        "  for w in doc:\n",
        "\n",
        "    str += w + \" \"\n",
        "  Xdoc.append(str)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the vectorizer to convert word lists to numeric vectors"
      ],
      "metadata": {
        "id": "PfcaE8_vNSHU"
      }
    },
    {
      "metadata": {
        "id": "RgWhDr0e6NdV"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "matrix = CountVectorizer(max_features=200)\n",
        "X = matrix.fit_transform(Xdoc).toarray()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePTLWaSC6Ngd",
        "outputId": "5711ca3e-4343-4d25-e1eb-efca588350a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   5    0    1 ...    1   34    4]\n",
            " [   6    0    2 ...    6   67    8]\n",
            " [   7    0    2 ...    6   85    8]\n",
            " ...\n",
            " [1811 1005 1072 ...  954 3609  925]\n",
            " [1814 1005 1073 ...  956 3621  925]\n",
            " [1817 1005 1077 ...  960 3634  925]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting the dataset into training and test sets."
      ],
      "metadata": {
        "id": "y25KiiCJNe8K"
      }
    },
    {
      "metadata": {
        "id": "EuoiMLybmZIL"
      },
      "cell_type": "code",
      "source": [
        "# split train and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r0XE9dGnoqtK",
        "outputId": "91c6e627-b5d8-4c74-88d5-205d1201b749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375\n",
            "125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using  Naive Bayes calssifier to create the model"
      ],
      "metadata": {
        "id": "SxPeh2pdN0o0"
      }
    },
    {
      "metadata": {
        "id": "gc1grK4XpyPU",
        "outputId": "54bcdf22-885b-4afd-e2ac-810c084491d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# predict class\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "\n",
        "print(y_test[5:15])\n",
        "print(y_pred[5:15])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learned', 'government', 'humor', 'reviews', 'mystery', 'reviews', 'hobbies', 'editorial', 'belles_lettres', 'news']\n",
            "['humor' 'hobbies' 'humor' 'reviews' 'lore' 'reviews' 'hobbies'\n",
            " 'editorial' 'belles_lettres' 'news']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the prediction made by the classifier"
      ],
      "metadata": {
        "id": "klDRSfUSOCRc"
      }
    },
    {
      "metadata": {
        "id": "Lil6ckfB4whv"
      },
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmWcXhwm47xQ",
        "outputId": "c8fe0085-97a0-4853-af15-9948bdda309b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "print(cr)\n",
        "print(cm)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "      adventure       0.69      1.00      0.82         9\n",
            " belles_lettres       1.00      0.71      0.83        21\n",
            "      editorial       0.50      1.00      0.67         2\n",
            "        fiction       0.80      1.00      0.89         8\n",
            "     government       1.00      0.70      0.82        10\n",
            "        hobbies       0.90      0.75      0.82        12\n",
            "          humor       0.36      1.00      0.53         4\n",
            "        learned       1.00      0.53      0.70        15\n",
            "           lore       0.71      1.00      0.83        12\n",
            "        mystery       1.00      0.60      0.75         5\n",
            "           news       1.00      0.70      0.82        10\n",
            "       religion       0.56      1.00      0.71         5\n",
            "        reviews       0.86      0.86      0.86         7\n",
            "        romance       1.00      0.25      0.40         4\n",
            "science_fiction       0.33      1.00      0.50         1\n",
            "\n",
            "       accuracy                           0.78       125\n",
            "      macro avg       0.78      0.81      0.73       125\n",
            "   weighted avg       0.87      0.78      0.78       125\n",
            "\n",
            "[[ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4 15  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  8  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  2  7  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  9  3  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  4  8  3  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 12  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  2  3  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  7  3  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  5  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1  6  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4sHIIjQQPSVA"
      },
      "cell_type": "markdown",
      "source": [
        "Assignment:\n",
        "\n",
        "1) Convert all the words to lower-case and then note any change in evaluation results.\n",
        "\n",
        "2) Apply Support Vector Machine (LinearSVM) classifier for training and then note any change in evaluation results.\n",
        "\n",
        "3) Remove stop words & punctuations and then note any change in evaluation results.\n",
        "\n",
        "4) Apply Support Vector Machine (LinearSVM) classifier for training and then note any change in evaluation results.\n",
        "\n",
        "[Before going to task 5 below, comment the changes made in task 4]\n",
        "\n",
        "5) Use Tf-idf vectorizer and then note any change in evaluation results.\n",
        "\n",
        "6) Then apply Support Vector Machine (LinearSVM) classifier for training and then note any change in evaluation results."
      ]
    },
    {
      "metadata": {
        "id": "tZ9rKfRQFd9s"
      },
      "cell_type": "markdown",
      "source": []
    }
  ]
}